{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####\n",
    "1. PyTorch предоставляет структуру данных Tensor, которая очень похожа на Numpy nndaray.\n",
    "2. Разбивает вычисления на части, которые считаются последовательно(представляется это ввиде динамического графа, где каждая вершина независимый код, обрабатывающий свою операцию). Все сложные вычисления происходят на С/С++\n",
    "3. Каждый torch.Tensor содержит компоненты:\n",
    " 3.1 grad (значение градиента) или\n",
    " 3.2 grad_fn(ссылка на функцию вычисления градиента)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Если для тензора нужно вычислить градиент, тогда requires_grad нужно присвоить значение True, по умолчанию False. Создается динамический граф, в котором автоматически создаются обратные функции, и при вызове .backward() запустится проход по графу в обратную сторону и сосчитается grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3374, -1.1522,  0.1524], requires_grad=True)\n",
      "tensor([-6.6871, -5.7611,  0.7618], grad_fn=<MulBackward0>)\n",
      "tensor(-3.8955, grad_fn=<MeanBackward0>)\n",
      "tensor([1.6667, 1.6667, 1.6667])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x*5\n",
    "print(y)\n",
    "\n",
    "z = y.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "tensor([5., 5., 5.], grad_fn=<MulBackward0>)\n",
      "tensor([9., 9., 9.], grad_fn=<AddBackward0>)\n",
      "tensor([5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x*5\n",
    "print(y)\n",
    "\n",
    "g = y+4\n",
    "print(g)\n",
    "\n",
    "g.backward(torch.ones_like(g))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Также с помощью пакета autograd можно вычислять jacobian, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import jacobian\n",
    "\n",
    "def exp(x):\n",
    "   return x.exp()\n",
    "\n",
    "X = torch.rand(2, 2)\n",
    "jacobian(exp, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-3., -2.,  2.,  5.]),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f(x) = x^2\n",
    "a = (torch.tensor([-2., -1., 1., 4.]),)\n",
    "b = torch.tensor([4., 1., 1., 16.], )\n",
    "torch.gradient(b, spacing=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5000, -0.2500, -0.2500, -0.5000]),)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([2., 1., 1., 0.], )\n",
    "torch.gradient(b, spacing=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autogradient for previous task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "\n",
    "# helpers\n",
    "f_calls_counter = 0\n",
    "grad_calls_counter = 0\n",
    "cur_lr = 0\n",
    "\n",
    "def get_min_by_f(f, eps, lr, x, start_lr = 0.1) :\n",
    "    global cur_lr\n",
    "    cur_lr = start_lr\n",
    "    points = np.asarray([])\n",
    "    counter = 0\n",
    "    next = next_x(x, lr, f, counter)\n",
    "    while LA.norm(next - x) > eps:\n",
    "        counter += 1\n",
    "        points = np.append(points, x)\n",
    "        x = next\n",
    "        next = next_x(x, lr, f, counter, eps)\n",
    "    return [x, counter, points]\n",
    "\n",
    "# multidimension function in point [x_0, ..., x_n]\n",
    "def f(x):\n",
    "    a = x[0]\n",
    "    return a**2 + 5*a + 10  \n",
    "\n",
    "def next_x(x, lr, f, epoch = 0, eps = 0.0001) : \n",
    "    global cur_lr \n",
    "    cur_lr = lr(cur_lr, epoch)\n",
    "    return x - cur_lr * grad(x, f, eps)    \n",
    "\n",
    "def grad(x, f, eps):\n",
    "        derivative = np.zeros(np.size(x))\n",
    "        for i in range(np.size(x)):\n",
    "            x[i] += eps\n",
    "            f1 = f(x)\n",
    "            x[i] -= 2 * eps\n",
    "            f2 = f(x)\n",
    "            x[i] += eps\n",
    "            derivative[i] = (f1 - f2) / (2 * eps)\n",
    "        return derivative   \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.003989696502685547 seconds\n",
      "epoch:  39\n",
      "min : [-2.49999721]\n",
      "f(x): 3.750000000007783\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "    \n",
    "start_time = time.time()    \n",
    "answer = get_min_by_f(f, 1e-6, (lambda lr, epoch: 0.16), np.asarray([7.0]))\n",
    "print(\" %s seconds\" % (time.time() - start_time))\n",
    "print(\"epoch: \", answer[1])\n",
    "print(\"min :\", answer[0])   \n",
    "print(\"f(x):\", f(answer[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_gd(f, eps, lr, x_, start_lr = 0.16, max_epoch=1000):\n",
    "    points = np.zeros((max_epoch, 1))\n",
    "    x = [x_]\n",
    "    points[0] = x[0].detach().numpy()\n",
    "    cur_lr = start_lr\n",
    "    i=0\n",
    "    while i<max_epoch-1:\n",
    "        i += 1\n",
    "        t = f(x[-1])\n",
    "        t.backward()\n",
    "        cur_lr = lr(cur_lr, 0)\n",
    "        points[i] = x[-1].detach().numpy() - cur_lr * x[-1].grad.numpy()\n",
    "        x.append(torch.tensor(points[i], requires_grad=True))\n",
    "        if (LA.norm(points[i]-points[i-1])<eps): break\n",
    "    return [x, i, points]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.015913963317871094 seconds\n",
      "epoch:  40\n",
      "min : tensor([-2.5000], dtype=torch.float64, requires_grad=True)\n",
      "f(x): tensor(3.7500, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()    \n",
    "answer = torch_gd(f, 1e-6, (lambda lr, epoch: 0.16), x_=torch.tensor([7.0], requires_grad=True))\n",
    "print(\" %s seconds\" % (time.time() - start))\n",
    "print(\"epoch: \", answer[1])\n",
    "print(\"min :\", answer[0][answer[1]])  \n",
    "print(\"f(x):\", f(answer[0][answer[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2. Использование вариантов SGD (torch.optim) из PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LR = 0.01\n",
    "njev = 0\n",
    "nfev = 0\n",
    "\n",
    "constant_lr = (lambda current_lr, epoch: DEFAULT_LR)\n",
    "\n",
    "def step_decay(lr, epoch, epochs_drop = 20, drop = 0.5):\n",
    "   return lr * pow(drop, floor((1 + epoch) / epochs_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear function, computes k-th coordinate by k-1 first\n",
    "#   coefficients - k-1-dim array\n",
    "#   x - k-1-dim array\n",
    "def f(coefficients, x, delta = 0.):\n",
    "    return np.sum(coefficients * x) + delta\n",
    "\n",
    "class point_set:\n",
    "    def __init__(self, n, dim, x, y, coefficients, delta, coords_bound = 0.):\n",
    "        self.n = n\n",
    "        self.dim = dim\n",
    "        self.x = np.asarray(x, dtype=np.float64)\n",
    "        self.y = np.asarray(y, dtype=np.float64)\n",
    "        self.coefficients = coefficients\n",
    "        self.delta = delta\n",
    "        self.coords_bound = coords_bound\n",
    "\n",
    "# create point_set:\n",
    "# n - amount of points\n",
    "# dim - space's dim\n",
    "# coords_bound - upper bound on absolute first k-1 coordinates value\n",
    "# coefficients_bound - upper bound on absolute coefficients value\n",
    "def create_point_set(n, dim, coords_bound, coefficients_bound, coefficients = []):\n",
    "    x = coords_bound * np.random.random_sample((n, dim - 1))\n",
    "    if coefficients == []:\n",
    "        coefficients = coefficients_bound * np.random.random_sample((dim - 1)) - coefficients_bound\n",
    "    else:\n",
    "        coefficients = np.asarray(coords_bound)\n",
    "\n",
    "    y = np.asarray([])\n",
    "    delta = np.asarray([0] * n)\n",
    "    for i in range(0, n):\n",
    "        delta[i] = coefficients_bound * (2 * np.random.random_sample() - 1)\n",
    "        y = np.append(y, f(coefficients, x[i], delta[i]))\n",
    "    return point_set(n, dim, x, y, coefficients, delta, coords_bound)\n",
    "\n",
    "\n",
    "EPOCH_MAX = 500\n",
    "input_2d = create_point_set(200, 2, 20., 10.)\n",
    "input_large = create_point_set(200, 10, 20., 10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(points, coeffs, i):\n",
    "    return 1 / (points.n) * np.square(coeffs[points.dim - 1] + np.sum(coeffs[0 : points.dim - 1] * points.x[i]) - points.y[i]) \n",
    "\n",
    "def apply_all_loss_functions(points, coeffs):\n",
    "    result = 0.\n",
    "    for i in range(0, points.dim):\n",
    "        result += loss_function(points, coeffs, i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_compute(points, coeffs, id, h=1e-5):\n",
    "    dim = points.dim\n",
    "    grad = np.asarray([0.] * dim)\n",
    "    step = np.asarray([0.] * dim)\n",
    "    for i in range(0, dim):\n",
    "        step[i] = h\n",
    "        grad[i] = (loss_function(points, coeffs + step, id) - loss_function(points, coeffs - step, id)) / (2 * h)\n",
    "        step[i] = 0.\n",
    "    return grad\n",
    "\n",
    "def loss_function_grad(points, ids, coeffs):\n",
    "    result = np.asarray([0.] * points.dim)\n",
    "    for i in ids:\n",
    "        result += gradient_compute(points, coeffs, i)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(points, batch_size, gradinet_supplier, starting_coeffs, lr, current_lr, eps, epoches, _param_1, _param_2):\n",
    "    total_epoches = 0\n",
    "    loss_history = list()\n",
    "    order = np.asarray(range(0, points.n))\n",
    "    np.random.shuffle(order)\n",
    "\n",
    "    batch_index = 0\n",
    "    coeffs = starting_coeffs\n",
    "    grad = np.asarray([0] * points.dim)\n",
    "    \n",
    "    for i in range(0, epoches):\n",
    "        to = min(batch_index + batch_size, len(order))\n",
    "        current_lr = lr(current_lr, i)\n",
    "        grad = gradinet_supplier(points, order[batch_index : to], coeffs)\n",
    "        coeffs_step = coeffs - current_lr * np.asarray(grad)\n",
    "        \n",
    "        if LA.norm(coeffs_step - coeffs) <= eps:\n",
    "            break\n",
    "        \n",
    "        total_epoches += 1\n",
    "        loss_history.append(apply_all_loss_functions(points, coeffs))\n",
    "        coeffs = coeffs_step\n",
    "        \n",
    "        if to == len(order):\n",
    "            batch_index = 0\n",
    "            np.random.shuffle(order)\n",
    "        else:\n",
    "            batch_index = to\n",
    "\n",
    "    return [coeffs, total_epoches, loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_with_momentum(points, batch_size, gradinet_supplier, starting_coeffs, lr, current_lr, eps, epoches, betta, param_2):\n",
    "    total_epoches = 0\n",
    "    loss_history = list()\n",
    "    order = np.asarray(range(0, points.n))\n",
    "    np.random.shuffle(order)\n",
    "\n",
    "    coeffs = starting_coeffs\n",
    "    grad = np.asarray([0] * points.dim)\n",
    "    batch_index = 0\n",
    "    \n",
    "    for i in range(0, epoches):\n",
    "        to = min(batch_index + batch_size, len(order))\n",
    "        current_lr = lr(current_lr, i)\n",
    "        grad = betta * grad + (1 - betta) * gradinet_supplier(points, order[batch_index:to], coeffs)\n",
    "        coeffs_step = coeffs - current_lr * grad\n",
    "        \n",
    "        if LA.norm(coeffs_step - coeffs) <= eps:\n",
    "            break\n",
    "        \n",
    "        total_epoches += 1\n",
    "        loss_history.append(apply_all_loss_functions(points, coeffs))\n",
    "        coeffs = coeffs_step\n",
    "        \n",
    "        if to == len(order):\n",
    "            batch_index = 0\n",
    "            np.random.shuffle(order)\n",
    "        else:\n",
    "            batch_index = to\n",
    "\n",
    "    return [coeffs, total_epoches, loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_loss_function_grad(points, ids, coeffs, betta, grad):\n",
    "    result = np.asarray([0.] * points.dim)\n",
    "    for i in ids:\n",
    "        result += gradient_compute(points, coeffs - betta * grad, i)\n",
    "    return result\n",
    "\n",
    "def sgd_with_nesterov_momentum(points, batch_size, _gradinet_supplier, starting_coeffs, lr, current_lr, eps, epoches, betta, gamma):\n",
    "    total_epoches = 0\n",
    "    loss_history = list()\n",
    "    order = np.asarray(range(0, points.n))\n",
    "    np.random.shuffle(order)\n",
    "\n",
    "    coeffs = starting_coeffs\n",
    "    grad = np.asarray([0] * points.dim)\n",
    "    batch_index = 0\n",
    "\n",
    "    for i in range(0, epoches):\n",
    "        to = min(batch_index + batch_size, len(order))\n",
    "        current_lr = lr(current_lr, i)\n",
    "        grad = betta * grad + current_lr * nesterov_loss_function_grad(points, order[batch_index:to], coeffs, betta, grad)\n",
    "        coeffs_step = coeffs - grad\n",
    "        \n",
    "        if LA.norm(coeffs_step - coeffs) <= eps:\n",
    "            break\n",
    "        \n",
    "        total_epoches += 1\n",
    "        coeffs = coeffs_step\n",
    "        loss_history.append(apply_all_loss_functions(points, coeffs))\n",
    "\n",
    "        if to == len(order):\n",
    "            batch_index = 0\n",
    "            np.random.shuffle(order)\n",
    "        else:\n",
    "            batch_index = to\n",
    "\n",
    "    return [coeffs, total_epoches, loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(points, batch_size, gradinet_supplier, starting_coeffs, lr, current_lr, eps, epoches, betta, gamma):\n",
    "    total_epoches = 0\n",
    "    loss_history = list()\n",
    "    order = np.asarray(range(0, points.n))\n",
    "    np.random.shuffle(order)\n",
    "\n",
    "    batch_index = 0\n",
    "    coeffs = starting_coeffs\n",
    "    grad = np.asarray([0] * points.dim)\n",
    "    prev_v= np.asarray([0] * points.dim)\n",
    "    \n",
    "    \n",
    "    for i in range(0, epoches):\n",
    "        to = min(batch_index + batch_size, len(order))\n",
    "        current_lr = lr(current_lr, i)\n",
    "        grad = gradinet_supplier(points, order[batch_index:to], coeffs)\n",
    "        v = prev_v + np.power(grad, 2)\n",
    "        coeffs_step = coeffs - np.divide(grad, np.power(v, 0.5)) * current_lr\n",
    "        \n",
    "        if LA.norm(coeffs_step - coeffs) <= eps:\n",
    "            break\n",
    "        \n",
    "        total_epoches += 1\n",
    "        loss_history.append(apply_all_loss_functions(points, coeffs))\n",
    "        coeffs = coeffs_step\n",
    "        prev_v = v\n",
    "        \n",
    "        if to == len(order):\n",
    "            batch_index = 0\n",
    "            np.random.shuffle(order)\n",
    "        else:\n",
    "            batch_index = to\n",
    "\n",
    "    return [coeffs, total_epoches, loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_prop(points, batch_size, gradinet_supplier, starting_coeffs, lr, current_lr, eps, epoches, betta, gamma):\n",
    "    total_epoches = 0\n",
    "    loss_history = list()\n",
    "    order = np.asarray(range(0, points.n))\n",
    "    np.random.shuffle(order)\n",
    "\n",
    "    batch_index = 0\n",
    "    coeffs = starting_coeffs\n",
    "    grad = np.asarray([0] * points.dim)\n",
    "    prev_v= np.asarray([0] * points.dim)\n",
    "    \n",
    "    \n",
    "    for i in range(0, epoches):\n",
    "        to = min(batch_index + batch_size, len(order))\n",
    "        current_lr = lr(current_lr, i)\n",
    "        grad = gradinet_supplier(points, order[batch_index:to], coeffs)\n",
    "        v = prev_v * betta + (1 - betta) * np.power(grad, 2)\n",
    "        coeffs_step = coeffs - np.divide(grad, np.power(v, 0.5)) * current_lr\n",
    "        \n",
    "        if LA.norm(coeffs_step - coeffs) <= eps:\n",
    "            break\n",
    "        \n",
    "        total_epoches += 1\n",
    "        loss_history.append(apply_all_loss_functions(points, coeffs))\n",
    "        coeffs = coeffs_step\n",
    "        prev_v = v\n",
    "        \n",
    "        if to == len(order):\n",
    "            batch_index = 0\n",
    "            np.random.shuffle(order)\n",
    "        else:\n",
    "            batch_index = to\n",
    "\n",
    "    return [coeffs, total_epoches, loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(points, batch_size, gradinet_supplier, starting_coeffs, lr, current_lr, eps, epoches, betta, gamma):\n",
    "    total_epoches = 0\n",
    "    loss_history = list()\n",
    "    order = np.asarray(range(0, points.n))\n",
    "    np.random.shuffle(order)\n",
    "\n",
    "    batch_index = 0\n",
    "    coeffs = starting_coeffs\n",
    "    prev_grad = np.asarray([0] * points.dim)\n",
    "    prev_v= np.asarray([0] * points.dim)\n",
    "    epsilon = np.asarray([0.01] * points.dim)\n",
    "    \n",
    "    \n",
    "    for i in range(0, epoches):\n",
    "        to = min(batch_index + batch_size, len(order))\n",
    "        current_lr = lr(current_lr, i)\n",
    "        \n",
    "        grad = gradinet_supplier(points, order[batch_index:to], coeffs)\n",
    "        \n",
    "        grad_current = betta * prev_grad + (1-betta) * grad\n",
    "        v = prev_v*gamma + (1-gamma)*np.power(grad, 2)\n",
    "\n",
    "        coeffs_step = coeffs - np.divide(grad_current, (np.power(v, 0.5) + epsilon))*current_lr\n",
    "        \n",
    "        if LA.norm(coeffs_step - coeffs) <= eps:\n",
    "            break\n",
    "        \n",
    "        total_epoches += 1\n",
    "        loss_history.append(apply_all_loss_functions(points, coeffs))\n",
    "        coeffs = coeffs_step\n",
    "        prev_v = v\n",
    "        prev_grad = grad_current\n",
    "        \n",
    "        if to == len(order):\n",
    "            batch_index = 0\n",
    "            np.random.shuffle(order)\n",
    "        else:\n",
    "            batch_index = to\n",
    "\n",
    "    return [coeffs, total_epoches, loss_history]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_to_array(model):\n",
    "    return np.append(model.weight.detach().numpy()[0], model.bias.detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(coeffs, points):\n",
    "    y = np.asarray([0.] * points.n)\n",
    "    for i in range(0, points.n):\n",
    "        y[i] = coeffs[points.dim - 1] + np.sum(coeffs[0 : points.dim - 1].detach().numpy() * points.x[i])\n",
    "    return torch.tensor(y, requires_grad=True)\n",
    "\n",
    "# points - point_set instance\n",
    "def torch_sgd(points, model, optimizer, scheduler, eps):\n",
    "    xs = torch.tensor(points.x, requires_grad=True).float()\n",
    "    ys = torch.tensor(np.asarray([points.y]).T).float()\n",
    "\n",
    "    loss_history = list()\n",
    "    error_history = list()\n",
    "    total_epoches = 0\n",
    "    \n",
    "    previous_coeffs = np.zeros(points.dim)\n",
    "\n",
    "    for epoch in range(0, EPOCH_MAX):\n",
    "        coeffs = model(xs)\n",
    "        loss = torch.nn.functional.mse_loss(coeffs, ys)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_coeffs = Linear_to_array(model)\n",
    "        if LA.norm(current_coeffs - previous_coeffs) <= eps:\n",
    "            break\n",
    "        \n",
    "        previous_coeffs = current_coeffs\n",
    "\n",
    "        total_epoches += 1\n",
    "        loss_history.append(float(loss) / points.n)\n",
    "        \n",
    "    return [Linear_to_array(model), total_epoches, loss_history]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_2d_result(points, result):\n",
    "    plt.plot(points.x, points.y, 'o', markersize=1)\n",
    "    grid = np.linspace(np.min(points.x), np.max(points.x), 1000)\n",
    "    plt.title('Point set and evaluated function')\n",
    "    plt.plot(grid, result[0][0] * grid + result[0][1])\n",
    "    plt.show()\n",
    "\n",
    "def print_result(points, result, tittle=''):\n",
    "    print(tittle, \"sgd result:\")\n",
    "    print(\"coeffs:\", result[0])\n",
    "    print(\"epoches:\", result[1])\n",
    "    print(\"loss:\", result[2][-1])\n",
    "    \n",
    "    if points.dim == 2:\n",
    "        draw_2d_result(points, result)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "def draw_loss(custom_result, result):\n",
    "    plt.plot(custom_result, label='custom loss')\n",
    "    plt.plot(result, label='torch loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sgd(points, optimizer_getter, custom_gd, current_lr, eps = 1e-6, param_1 = 0, param_2 = 0):\n",
    "\n",
    "    model = torch.nn.Linear(points.dim - 1, 1)\n",
    "    starting_coeffs = Linear_to_array(model)\n",
    "\n",
    "    optimizer = optimizer_getter(model)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2, 0.5)\n",
    "    \n",
    "    custom_result = custom_gd(points, points.n * 2 // 5, loss_function_grad, starting_coeffs, step_decay, current_lr, eps, EPOCH_MAX, param_1, param_2)    \n",
    "    result = torch_sgd(points, model, optimizer, scheduler, eps)\n",
    "\n",
    "    print_result(points, custom_result, 'custom')\n",
    "    print_result(points, result, 'torch')\n",
    "    draw_loss(custom_result[2], result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sgd(input_2d, lambda model: torch.optim.SGD(model.parameters(), lr=DEFAULT_LR), sgd, DEFAULT_LR)\n",
    "print(\"===large input===\\n\")\n",
    "compare_sgd(input_large, lambda model: torch.optim.SGD(model.parameters(), lr=0.001), sgd, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sgd(input_2d, lambda model: torch.optim.SGD(model.parameters(), lr=DEFAULT_LR, momentum=0.7), sgd_with_momentum, DEFAULT_LR, param_1=0.7)\n",
    "print(\"===large input===\\n\")\n",
    "compare_sgd(input_large, lambda model: torch.optim.SGD(model.parameters(), lr=DEFAULT_LR, momentum=0.8), sgd_with_momentum, DEFAULT_LR, param_1=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD with Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sgd(input_2d, lambda model: torch.optim.SGD(model.parameters(), lr=DEFAULT_LR, momentum=0.7, nesterov=True), sgd_with_nesterov_momentum, DEFAULT_LR, param_1=0.7)\n",
    "print(\"===large input===\\n\")\n",
    "compare_sgd(input_large, lambda model: torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True), sgd_with_nesterov_momentum, 0.001, param_1=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sgd(input_2d, lambda model: torch.optim.Adagrad(model.parameters(), lr=2), adagrad, 2)\n",
    "print(\"===large input===\\n\")\n",
    "compare_sgd(input_large, lambda model: torch.optim.Adagrad(model.parameters(), lr=1.5), adagrad, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMS Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sgd(input_2d, lambda model: torch.optim.RMSprop(model.parameters(), lr=1), rms_prop, 1, param_1=0.99)\n",
    "print(\"===large input===\\n\")\n",
    "compare_sgd(input_large, lambda model: torch.optim.RMSprop(model.parameters(), lr=1.5, momentum=0.8), rms_prop, 1.5, param_1=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sgd(input_2d, lambda model: torch.optim.Adam(model.parameters(), lr=1), adam, 1, param_1=0.9, param_2=0.999)\n",
    "print(\"===large input===\\n\")\n",
    "compare_sgd(input_large, lambda model: torch.optim.Adam(model.parameters(), lr=1, betas=(0.8, 0.8)), adam, 1, param_1=0.8, param_2=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import *\n",
    "from scipy.optimize import minimize, least_squares\n",
    "from autograd import grad, jacobian\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "from abc import abstractmethod\n",
    "from numpy import matmul\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy.optimize.leas_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "def hess(f):\n",
    "    J = jacobian(f)\n",
    "    return 2 * J.T * J\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_x = [100., -3.]\n",
    "print(\"sample\")\n",
    "print(minimize(rosenbrock, start_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_with_grad(x):\n",
    "    x = torch.tensor(x, requires_grad=True)\n",
    "    res = rosenbrock(x)\n",
    "    res.backward()\n",
    "    return res.data.cpu().numpy(), x.grad.data.cpu().numpy()\n",
    "\n",
    "print(\"with gradient\")\n",
    "print(minimize(rosenbrock_with_grad, start_x, jac=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"with bounds\")\n",
    "print(minimize(rosenbrock, start_x, bounds = ((13, None), (-5, -2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixSolver:\n",
    "    # compute gradinent of function f in point x with step h\n",
    "    @staticmethod\n",
    "    def grad(x, f, eps):\n",
    "        derivative = np.zeros(np.size(x))\n",
    "        for i in range(np.size(x)):\n",
    "            x[i] += eps\n",
    "            f1 = f(x)\n",
    "            x[i] -= 2 * eps\n",
    "            f2 = f(x)\n",
    "            x[i] += eps\n",
    "            derivative[i] = (f1 - f2) / (2 * eps)\n",
    "        return derivative\n",
    "\n",
    "    # compute Jacobian of functions rs = (r_1...r_n), r_i = r_i(x_1...x_m)\n",
    "    @staticmethod\n",
    "    def findJacobian(rs, x, eps):\n",
    "        n = np.size(rs)\n",
    "        m = np.size(x)\n",
    "        J = np.zeros((n, m))\n",
    "        for i in range(n):\n",
    "            J[i] = MatrixSolver.grad(x, rs[i], eps)\n",
    "\n",
    "        return J\n",
    "\n",
    "    # compute pseudo inverse of matrix X\n",
    "    @staticmethod\n",
    "    def pseudoInverse(x):\n",
    "        return np.linalg.inv(x.T @ x) @ x.T\n",
    "\n",
    "    # compute hessian of functions R^n -> R\n",
    "    @staticmethod\n",
    "    def findHessian(func, x, eps):\n",
    "        J = MatrixSolver.findJacobian(func, x, eps)\n",
    "        return 2 * J.T @ J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher:\n",
    "    def __init__(self, eps, max_iterations, func, start_x):\n",
    "        self.eps = eps\n",
    "        self.max_iterations = max_iterations\n",
    "        self.func = func\n",
    "        self.start_x = start_x\n",
    "\n",
    "    # stop method\n",
    "    def is_not_final(self):\n",
    "        return LA.norm(self.next - self.cur_x) > self.eps and self.max_iterations >= self.epoch\n",
    "\n",
    "    # draw plot\n",
    "    def draw(self):\n",
    "        # print(\"points:\", self.points)\n",
    "\n",
    "        min_point = self.points[-1]\n",
    "        print(\"minimum:\", min_point)\n",
    "        \n",
    "        repoints = np.asarray(self.points).reshape(-1, 2)\n",
    "        pic = plt.figure(figsize=(10, 10))\n",
    "        offset = max(np.max(repoints[:, 0]), abs(np.min(repoints[:, 0])))\n",
    "        xs = np.linspace(min_point[0] - offset, min_point[0] + offset, 1000)\n",
    "\n",
    "        plt.plot(xs, [self.func([x]) for x in xs])\n",
    "        plt.plot(repoints[:, 0], repoints[:, 1], 'o-', color=\"red\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def save_point(self, x):\n",
    "        self.points.append(np.append(x, self.func([x])))\n",
    "\n",
    "    # executor method\n",
    "    def run(self, drawing = 1):\n",
    "        self.epoch = 2\n",
    "        self.cur_x = self.start_x\n",
    "        self.points = list()\n",
    "        self.save_point(self.cur_x)\n",
    "    \n",
    "        self.next_point()\n",
    "        while (self.is_not_final()):\n",
    "            self.epoch += 1\n",
    "            self.cur_x = self.next\n",
    "            self.save_point(self.cur_x)\n",
    "            self.next_point()\n",
    "\n",
    "        self.save_point(self.next)\n",
    "        print(\"argmin: \", self.next, \"\\nepoches: \", self.epoch, \"\\nnfev: \", self.epoch * 2)\n",
    "\n",
    "        if drawing == 1:\n",
    "            self.draw()\n",
    "\n",
    "\n",
    "    # find next point\n",
    "    @abstractmethod\n",
    "    def next_point(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dogleg(Searcher):\n",
    "\n",
    "    def __init__(self, eps, max_iteration, func, start_x,\n",
    "                 initial_trust_radius=1.0, max_trust_radius=100.0, eta = 0.15):\n",
    "        super().__init__(eps, max_iteration, func, start_x)\n",
    "        self.initial_trust_radius = initial_trust_radius\n",
    "        self.max_trust_radius = max_trust_radius\n",
    "        self.eta = eta\n",
    "\n",
    "    def find_shift(self):\n",
    "        # find optimum\n",
    "        optimum = -(np.linalg.inv(self.hess) @ self.jac)\n",
    "        norm_opt = sqrt(optimum @ optimum)\n",
    "\n",
    "        if norm_opt <= self.trust_radius:\n",
    "            return optimum\n",
    "\n",
    "        # find Cauchy point\n",
    "        cauchy = - (self.jac @ self.jac / (self.jac @ (self.hess @ self.jac))) @ self.jac\n",
    "        norm_cauchy = sqrt(cauchy @ cauchy)\n",
    "\n",
    "        # stop in circul\n",
    "        if norm_cauchy >= self.trust_radius:\n",
    "            return self.trust_radius * cauchy / norm_cauchy\n",
    "\n",
    "        tau = (self.trust_radius - cauchy) / (optimum - cauchy)\n",
    "\n",
    "        return cauchy + tau * (optimum - cauchy)\n",
    "\n",
    "    def find_trust_radius(self, shift):\n",
    "        act_reduction = self.func(self.cur_x) - self.func(self.cur_x + shift)\n",
    "        pred_reduction = -(self.jac @ shift + 0.5 * (shift @ (self.hess @ shift)))\n",
    "\n",
    "        rho = act_reduction / pred_reduction\n",
    "        if pred_reduction == 0.0:\n",
    "            rho = 1e99\n",
    "\n",
    "        norm_shift = sqrt(shift @ shift)\n",
    "\n",
    "        if rho < 0.25:\n",
    "            self.trust_radius = 0.25 * norm_shift\n",
    "        else:\n",
    "            if rho > 0.75 and norm_shift == self.trust_radius:\n",
    "                self.trust_radius = min(2.0 * self.trust_radius, self.max_trust_radius)\n",
    "            else:\n",
    "                self.trust_radius = self.trust_radius\n",
    "\n",
    "\n",
    "        if rho > self.eta:\n",
    "            self.next = self.cur_x + shift\n",
    "        else:\n",
    "            # too close\n",
    "            self.next = self.cur_x\n",
    "\n",
    "\n",
    "    def next_point(self):\n",
    "        self.jac = MatrixSolver.findJacobian([self.func], self.cur_x, self.eps)\n",
    "        self.hess = MatrixSolver.findHessian([self.func], self.cur_x, self.eps)\n",
    "        self.trust_radius = self.initial_trust_radius\n",
    "\n",
    "        self.find_trust_radius(self.find_shift())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussNewton(Searcher):\n",
    "\n",
    "    def calculate(self, r, x):\n",
    "        n = len(r)\n",
    "        res = np.zeros(n)\n",
    "        for i in range(n):\n",
    "          res[i] = r[i](x)\n",
    "        return res\n",
    "\n",
    "    def get_function(self, r):\n",
    "        return lambda x: self.calculate(r, x)\n",
    "\n",
    "    def __init__(self, eps, max_iteration, rs, start_x):\n",
    "        super().__init__(eps, max_iteration, self.get_function(rs), start_x)\n",
    "        self.rs = rs\n",
    "\n",
    "    def next_point(self):\n",
    "        p = MatrixSolver.pseudoInverse(MatrixSolver.findJacobian(self.rs, self.cur_x, self.eps))\n",
    "        #print(self.cur_x)\n",
    "        self.next = self.cur_x - p @ self.func(self.cur_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFGS(Searcher):\n",
    "\n",
    "    def __init__(self, eps, max_iteration, func, start_x, func_grad):\n",
    "        super().__init__(eps, max_iteration, func, start_x)\n",
    "        self.func_grad = func_grad\n",
    "        self.H = np.eye(len(self.start_x), len(self.start_x))\n",
    "\n",
    "    def next_H(self, y, s):\n",
    "        I = np.eye(len(s), len(s)) # единичная матрица\n",
    "        y_t = np.transpose(y)\n",
    "        s_t = np.transpose(s)\n",
    "\n",
    "        g= 1.0/(y_t @ s)\n",
    "        g_s_y_T=g * (s @ y_t)\n",
    "        g_y_s_t=g * (y @ s_t)\n",
    "        return ((I-g_s_y_T) @ (self.H @ (I-g_y_s_t))) + g * (s @ s_t)\n",
    "\n",
    "\n",
    "    def next_point(self):\n",
    "        fgrad=MatrixSolver.grad(self.cur_x, self.func, self.eps)\n",
    "        p = -self.H @ fgrad\n",
    "\n",
    "        #находим alpha, которая подходит условиям вольфа\n",
    "        alpha = scipy.optimize.line_search(self.func, self.func_grad , self.cur_x, p, c1=1e-4, c2=0.9) # ищем коэф, удовлетворяющий условию Вольфе\n",
    "        if alpha[0] is None:\n",
    "            s = 1e-4 * p\n",
    "            print(\"None\")\n",
    "        else:\n",
    "            s = alpha[0] * p\n",
    "        \n",
    "        \n",
    "        # переход на следующую итерацию\n",
    "        self.next = self.cur_x + s\n",
    "        y=MatrixSolver.grad(self.next, self.func, self.eps)-fgrad\n",
    "        self.H = self.next_H(y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return sin(x[0]) + cos(x[0])\n",
    "\n",
    "start_x = [1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dogleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"from scipy.least_squares\")\n",
    "print(least_squares(f1, start_x, method=\"dogbox\", gtol=1e-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"from lab3\")\n",
    "Dogleg(1e-5, 1000, f1, np.array(start_x, float)).run(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_f1(x):\n",
    "    return torch.sin(x[0]) + torch.cos(x[0])\n",
    "\n",
    "def f1_with_grad(x):\n",
    "    x = torch.tensor(x, requires_grad=True)\n",
    "    res = torch_f1(x)\n",
    "    res.backward()\n",
    "    return res.data.cpu().numpy(), x.grad.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"from scipy.minimize\")\n",
    "print(minimize(f1_with_grad, start_x, method=\"Newton-CG\", jac=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"from lab3\")\n",
    "GaussNewton(1e-5, 1000, [f1], np.array([1.1], float)).run(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f1(x):\n",
    "    return cos(x[0]) - sin(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"from scipy.minimize\")\n",
    "print(minimize(f1_with_grad, start_x, method=\"BFGS\", jac=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"from lab3\")\n",
    "BFGS(1e-5, 1000, f1, np.array(start_x, float), grad_f1).run(0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "356e69385587f69b433e2dd0b10e19b52a7abdf22abb4354067a7098dda811b6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
