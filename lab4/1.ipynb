{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####\n",
    "1. PyTorch предоставляет структуру данных Tensor, которая очень похожа на Numpy nndaray.\n",
    "2. Разбивает вычисления на части, которые считаются последовательно(представляется это ввиде динамического графа, где каждая вершина независимый код, обрабатывающий свою операцию). Все сложные вычисления происходят на С/С++\n",
    "3. Каждый torch.Tensor содержит компоненты:\n",
    " 3.1 grad (значение градиента) или\n",
    " 3.2 grad_fn(ссылка на функцию вычисления градиента)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Если для тензора нужно вычислить градиент, тогда requires_grad нужно присвоить значение True, по умолчанию False. Создается динамический граф, в котором автоматически создаются обратные функции, и при вызове .backward() запустится проход по графу в обратную сторону и сосчитается grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4618,  0.6820, -1.1559], requires_grad=True)\n",
      "tensor([-2.3090,  3.4102, -5.7796], grad_fn=<MulBackward0>)\n",
      "tensor(-1.5595, grad_fn=<MeanBackward0>)\n",
      "tensor([1.6667, 1.6667, 1.6667])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x*5\n",
    "print(y)\n",
    "\n",
    "z = y.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "tensor([5., 5., 5.], grad_fn=<MulBackward0>)\n",
      "tensor([9., 9., 9.], grad_fn=<AddBackward0>)\n",
      "tensor([5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x*5\n",
    "print(y)\n",
    "\n",
    "g = y+4\n",
    "print(g)\n",
    "\n",
    "g.backward(torch.ones_like(g))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Также с помощью пакета autograd можно вычислять jacobian, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0068, 0.3230],\n",
      "        [0.6541, 0.0238]])\n",
      "tensor([[1.0068, 1.3812],\n",
      "        [1.9234, 1.0241]])\n"
     ]
    }
   ],
   "source": [
    "from autograd import jacobian\n",
    "\n",
    "def exp(x):\n",
    "   return x.exp()\n",
    "\n",
    "X = torch.rand(2, 2)\n",
    "jacobian(exp, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.5000, -0.7500,  3.7500,  7.5000]),)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f(x) = x^2\n",
    "a = (torch.tensor([-2., -1., 1., 4.]),)\n",
    "b = torch.tensor([4., 1., 1., 16.], )\n",
    "torch.gradient(b, spacing=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5000, -0.2500, -0.2500, -0.5000]),)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([2., 1., 1., 0.], )\n",
    "torch.gradient(b, spacing=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autogradient for previous task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "\n",
    "# helpers\n",
    "f_calls_counter = 0\n",
    "grad_calls_counter = 0\n",
    "cur_lr = 0\n",
    "\n",
    "def get_min_by_f(f, eps, lr, x, start_lr = 0.1) :\n",
    "    global cur_lr\n",
    "    cur_lr = start_lr\n",
    "    points = np.asarray([])\n",
    "    counter = 0\n",
    "    next = next_x(x, lr, f, counter)\n",
    "    while LA.norm(next - x) > eps:\n",
    "        counter += 1\n",
    "        points = np.append(points, x)\n",
    "        x = next\n",
    "        next = next_x(x, lr, f, counter, eps)\n",
    "    return [x, counter, points]\n",
    "\n",
    "# multidimension function in point [x_0, ..., x_n]\n",
    "def f(x):\n",
    "    a = x[0]\n",
    "    return a**2 + 5*a + 10  \n",
    "\n",
    "def next_x(x, lr, f, epoch = 0, eps = 0.0001) : \n",
    "    global cur_lr \n",
    "    cur_lr = lr(cur_lr, epoch)\n",
    "    return x - cur_lr * grad(x, f, eps)    \n",
    "\n",
    "def grad(x, f, eps):\n",
    "        derivative = np.zeros(np.size(x))\n",
    "        for i in range(np.size(x)):\n",
    "            x[i] += eps\n",
    "            f1 = f(x)\n",
    "            x[i] -= 2 * eps\n",
    "            f2 = f(x)\n",
    "            x[i] += eps\n",
    "            derivative[i] = (f1 - f2) / (2 * eps)\n",
    "        return derivative   \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.002953052520751953 seconds\n",
      "epoch:  39\n",
      "min : [-2.49999721]\n",
      "f(x): 3.750000000007783\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "    \n",
    "start_time = time.time()    \n",
    "answer = get_min_by_f(f, 1e-6, (lambda lr, epoch: 0.16), np.asarray([7.0]))\n",
    "print(\" %s seconds\" % (time.time() - start_time))\n",
    "print(\"epoch: \", answer[1])\n",
    "print(\"min :\", answer[0])   \n",
    "print(\"f(x):\", f(answer[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_gd(f, eps, lr, x_, start_lr = 0.16, max_epoch=1000):\n",
    "    points = np.zeros((max_epoch, 1))\n",
    "    x = [x_]\n",
    "    points[0] = x[0].detach().numpy()\n",
    "    cur_lr = start_lr\n",
    "    i=0\n",
    "    while i<max_epoch-1:\n",
    "        i += 1\n",
    "        t = f(x[-1])\n",
    "        t.backward()\n",
    "        cur_lr = lr(cur_lr, 0)\n",
    "        points[i] = x[-1].detach().numpy() - cur_lr * x[-1].grad.numpy()\n",
    "        x.append(torch.tensor(points[i], requires_grad=True))\n",
    "        if (LA.norm(points[i]-points[i-1])<eps): break\n",
    "    return [x, i, points]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.014671087265014648 seconds\n",
      "epoch:  40\n",
      "min : tensor([-2.5000], dtype=torch.float64, requires_grad=True)\n",
      "f(x): tensor(3.7500, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()    \n",
    "answer = torch_gd(f, 1e-6, (lambda lr, epoch: 0.16), x_=torch.tensor([7.0], requires_grad=True))\n",
    "print(\" %s seconds\" % (time.time() - start))\n",
    "print(\"epoch: \", answer[1])\n",
    "print(\"min :\", answer[0][answer[1]])  \n",
    "print(\"f(x):\", f(answer[0][answer[1]]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "356e69385587f69b433e2dd0b10e19b52a7abdf22abb4354067a7098dda811b6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
